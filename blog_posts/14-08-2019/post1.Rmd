---
title: "Função de Verossimilhança"
author: "Fernando de Souza Bastos"
#date: "`r format(Sys.time(), '%B %e, %Y')`"
date: "16 de agosto de 2019"
output:
    html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    toc: yes
    #Sumário flutuante
    #toc_float: true
    #numerar seções
    number_sections: true
    #Mostrar ou esconder os códigos (show ou hide)
    code_folding: hide
    #Diversos modelos de documentos ver outros em http://bootswatch.com/
    theme: united
    header-includes:
       \usepackage{array}
       \usepackage{multirow}
bibliography: bibfile.bib  
includes:
     keep_tex: yes
fontsize: 11pt
geometry: margin=1in
graphics: yes
#  pdf_document:
#    fig_caption: yes
#    keep_tex: yes
#    number_sections: yes
comments: yes
tags: [Distribuição Normal, otimização, R]
---

***

```{r,echo = FALSE}
options(OutDec=",")
```


```{r setup, include=FALSE}
require(knitr)
require(kfigr)
library(kableExtra)
options(knitr.table.format = "latex")
knitr::opts_chunk$set(echo = TRUE,fig.align = "center",message=FALSE, 
                      warning=FALSE,fig.height=5, fig.width=7)
```

# Introdução
<p style="text-align: justify;">
Seja $f(\mathbf{x}|\theta)$ a função densidade de probabilidade (fdp) ou função de 
probabilidade (fp) conjunta da amostra $X=(X_{1},\cdots,X_{n}).$ Então, dado que 
$X=x$ é observado, a função de $\theta$ definida por
$$
L(\theta|\mathbf{x})=f(\mathbf{x}|\theta)
$$
é chamada de função de verossimilhança. Se $X$ é um vetor aleatório discreto, então 
$L(\theta|\mathbf{x})=P_{\theta}(X=x).$ Se $X$ é uma variável aleatória contínua com valor real e se a fdp de
$X$ é contínua em $x,$ então, para $\epsilon$ pequeno, $P_{\theta}(x-\epsilon<X<x+\epsilon)\approx 
2\epsilon{}f(\mathbf{x}|\theta)=2\epsilon{}L(\theta|\mathbf{x}).$ Apesar de parecer que a função de 
verossmilhança é igual a uma fdp ou a fp, há uma distinção sútil, pois no caso da fdp ou fp consideramos 
$\theta$ fixo e $x$ variável. No caso da função de verossimilhança $L(\theta|\mathbf{x}),$ consideramos que 
$x$ é ponto amostral observado (fixo) e que $\theta$ é a variável assumindo valores em um espaço paramétrico 
$\Theta$ qualquer. 
</p>

<p style="text-align: justify;">
Considere uma variável aleatória $X$, cujo comportamento pode ser explicado por duas hipóteses (hipóteses $A$
e $B$) que desejamos comparar. Realizamos um estudo e observamos um valor $x$ de $X.$ O que as hipóteses 
dizem a respeito dessa observação?

- A hipótese $A$ implica que $X=x$ seria observado com probabilidade $p_{A}(x);$
- A hipótese $B$ implica que $X=x$ seria observado com probabilidade $p_{B}(x);$

No processo de investigação científica, no entanto, o que interessa é a pergunta: 

<p style="text-align: center;">
"*O que a observação de $X = x$ diz a respeito das hipóteses $A$ e $B?$*"
</p>

A Lei da Verossimilhança afirma que, no caso discreto, a observação $X = x$ é uma evidência que favorece a 
hipótese $A$ sobre a hipótese $B$ se, e somente se, 
\begin{align}
P_{A}(X=x)=L(A|x)>L(B|x)=P_{B}(X=x)
\end{align}

Mais ainda, a Lei da Verossimilhança implica que a *Razão de Verossimilhança*
\begin{align}
\dfrac{P_{A}(X=x)}{P_{B}(X=x)}=\dfrac{L(A|x)}{L(B|x)}
\end{align}
mede a *força de evidência* em favor da hipótese $A$ sobre a hipótese $B.$ No caso contínuo, escreveríamos,
$$
\dfrac{P_{A}(x-\epsilon<X<x+\epsilon)}{P_{B}(x-\epsilon<X<x+\epsilon)}\approx 
\dfrac{L(A|\mathbf{x})}{L(B|\mathbf{x})}.
$$

## Exemplo
<p style="text-align: justify;">
O número de mensagens eletrônicas (em centenas) recebidas por um provedor em horário comercial pode ser 
modelado por uma distribuição de probabilidade. Há duas hipóteses que iremos considerar:

- Hipótese A: o número médio de mensagens eletrônicas (em centenas) recebidas por dia é 15;

- Hipótese B: o número médio de mensagens  (em centenas) recebidas recebidas por dia é 25;

Observou-se, durante um dia qualquer, 2000 mensagens. Neste caso, o número de mensagens $X$ é uma variável 
aleatória e a distribuição de _Poisson_ é uma candidata para a modelagem, pois trata-se de dados de contagem.
Assim, se $X$ tem distribuição _Poisson_, sua função densidade é dada por:
$$
P(X=x)=\dfrac{e^{-\mu}\mu^{x}}{x!},
$$
em que $\mu$ é o número médio de mensagens. Logo, as probabilidades de acordo com as hipóteses são:

- Hipótese A: $\mu=15$
$$
p_{A}(20)=\dfrac{e^{-15}15^{20}}{20!}=0.04181031.
$$
- Hipótese B: $\mu=25$
$$
p_{B}(20)=\dfrac{e^{-25}25^{20}}{20!}=0.05191747.
$$
Logo, a observação $X=20$ favorece a hipótese $B$ sobre a hipótese $A.$ A *força de evidência* em favor de 
$B$ sobre $A$ é
$$
\dfrac{p_{B}(20)}{p_{A}(20)}=\dfrac{0.05191747}{0.04181031}=1.241738.
$$

Ou seja, pode se dizer que a observação $X = 20$ é evidência que a hipótese $B$ é aproximadamente 1.25 vezes
mais verossímil que a hipótese $A.$
</p>

## Discussão sobre exemplo anterior

<p style="text-align: justify;">
Notemos que ao comparar as hipóteses utilizamos a _função de densidade_ da distribuição _Poisson_:
$$
P(X=x)=\dfrac{e^{-\mu}\mu^{x}}{x!}
$$
em que $x$ é o valor de uma observação da variável aleatória Poisson e $\mu$ é o seu parâmetro. A observação $X = 20$ foi dada e, portanto, o valor de $x$ é conhecido e fixo. Assim, a função utilizada foi 
$$
P(X=20)=\dfrac{e^{-\mu}\mu^{20}}{20!}.
$$
</p>
<p style="text-align: justify;">
Essa expressão não é mais uma função do valor da observação $x,$ e sim uma função do parâmetro $\mu,$ que 
variou da hipótese $A$ para hipótese $B.$ 

Sempre que numa função densidade, ou função de probabilidade, a observação for fixada e o parâmetro 
desconhecido (variável), não se tem mais uma função densidade, mas uma função de verossimilhança. Esta última
indica a verossimilhança de uma dada hipótese, por exemplo a hipótese $A$ em que $\mu = 15$, dado que 
observou-se $X = 20$. Para tornar mais claro este conceito se utiliza uma notação diferente para a função de
verossimilhança:
$$
L(hipótese|dados)\ \textrm{ou}\  L(A|X=x)\ \textrm{ou}\ L(\mu|X=x)
$$

Assim, de acordo com o exemplo anterior, temos:
$$
L(\mu|X=20)=\dfrac{e^{-\mu}\mu^{20}}{20!}
$$

A `r figr("fig1", TRUE, type="Figura")` apresenta o gráfico dessa função para valores de $\mu$ entre 10 e 50.
Notemos que o valor da observação influencia fortemente o comportamento da função de verossimilhança, o que 
pode ser observado na `r figr("fig2", TRUE, type="Figura")`.
</p>
```{r fig1,echo=TRUE,anchor="Figura"}
x <- 20
mu <- seq(10, 50, l = 100)
lmu <- (exp(-mu)*(mu^x))/factorial(x)
expr = expression(L*(mu ~ "|" ~ X==20))
plot(mu, lmu, type = "l", xlab = expression(mu), ylab = expr, main = "Figura 1: Verossimilhança de uma 
     distribuição Poisson para x=20")
```

```{r fig2,echo=TRUE,anchor="Figura"}
mu <- seq(10, 50, l = 100)

lmu<-function(x,mu){
  lmu1<-(exp(-mu)*(mu^x))/factorial(x)
  return(lmu1)
}
expr = expression(L*(mu ~ "|" ~ X==x))
plot(mu, lmu(10,mu), type = "l", xlab = expression(mu), ylab = expr, main = "Figura 2: Verossimilhança para 
     diferentes valores observados de X")
lines(mu, lmu(13,mu),col=2,lty=2, lwd=1)
lines(mu, lmu(17,mu),col=3,lty=3, lwd=2)
lines(mu, lmu(20,mu),col=4,lty=4, lwd=3)
lines(mu, lmu(25,mu),col=5,lty=5, lwd=3)
lines(mu, lmu(40,mu),col=6,lty=6, lwd=2)
legend(40, 0.12, c(expression(x==10), 
                    expression(x==13),
                    expression(x==17),
                    expression(x==20),
                    expression(x==25),
                    expression(x==40)), 
       col=1:6, 
       lwd=c(1,1,2,3,3,2), 
       lty=c(1,2,3,4,5,6))
```

</p>

_________________________________________________

## Múltiplas Observações

Em geral, quando fazemos qualquer estudo estatístico, trabalhamos com várias observações independentes para 
compor uma amostra. Assim, ao assumir uma hipótese $A$ como verdadeira, a probabilidade de se obter uma 
amostra $X=(X_{1},\cdots,X_{n}),$ composta de $n$ observações independentes de $X$ é dada por
$$
P(X|A)=P(X=x_{1}|A)P(X=x_{2}|A)\dots{}P(X=x_{n}|A).
$$
Ou seja, a probabilidade de se obter a amostra observada, dado a hipótese $A,$ é igual ao produto das 
probabilidades das observações individuais, dado a hipótese $A.$ Da mesma forma, a função de verossimilhança
de uma amostra composta de observações independentes será o produto das funções de verossimilhança das 
observações individuais, ou seja,
\begin{align}
L(A|X)&=L(A|X=x_{1})L(A|X=x_{2})\dots L(A|X=x_{n})\\
&={\displaystyle \prod_{i=1}^{n}L(A|X=x_{i})}
\end{align}

## Exemplo

Suponha que tenhamos contabilizado durante 10 dias as mensagens recebidas. Os resultados são apresentados na
Tabela 1 abaixo. 
$$
\begin{array}{cccc}
\hline
Dia&N. de Mensagens&Verossimilhança&Verossimilhança\\
(i)&(X=x_{i})&\mu=15&\mu=25\\
\hline
1 &20&0.04181031  &0.05191747\\
2 &23&0.01327967  &0.07634203\\
3 &25&0.004979876 &0.07952295\\
4 &22&0.02036216  &0.07023467\\
5 &27&0.001596114 &0.07080035\\
6 &25&0.004979876 &0.07952295\\
7 &23&0.01327967  &0.07634203\\
8 &28&0.0008550612&0.0632146 \\
9 &29&0.000442273 &0.05449534\\
10&21&0.0298645   &0.06180651\\
\hline
&{\displaystyle \prod_{i=1}^{10}}&6.711593e-23&2.025927e-12\\
\hline
\end{array}
$$

<!--|Dia             |N. de Mensagens |Verossimilhança       |Verossimilhança|
|----------------|----------------|----------------------|---------------|
|$(i)$           |$(X=x_{i})$     |$\mu=15$              |$\mu=25$       |
|                |                |                      |               |
|1 |20|0,04181031  |0,05191747|
|2 |23|0,01327967  |0,07634203|
|3 |25|0,004979876 |0,07952295|
|4 |22|0,02036216  |0,07023467|
|5 |27|0,001596114 |0,07080035|
|6 |25|0,004979876 |0,07952295|
|7 |23|0,01327967  |0,07634203|
|8 |28|0,0008550612|0,0632146 |
|9 |29|0,000442273 |0,05449534|
|10|21|0,0298645   |0,06180651|
|  |${\displaystyle \prod_{i=1}^{10}L(\theta|X=x_{i})}$|6,711593e-23|2,025927e-12|
|                |                |                      |               |
-->

Observamos na Tabela acima que a verossimilhança apresenta um valor próximo de zero, isso porque a 
verossimilhança da amostra é o produto da verossimilhança das observações. Além disso, observamos que a 
hipótese $B$ permanece como a hipótese mais verossímil.

## Log-Verossimilhança

Na maioria dos casos, especialmente quando vamos diferenciar a função de verossimilhança, é mais fácil 
trabalhar com o logaritmo natural da função de verossimilhança do que trabalhar diretamente com 
$L(\theta|X).$ Assim, vamos utilizar 
$$
\mathcal{L}(\theta|X=x)=log{(L(\theta|X))}
$$

É fácil observar que o valor numérico da verossimilhança é geralmente (não necessariamente) menor do que um.
Logo, o logaritmo desse valor é negativo. Por isso, as vezes, trabalhamos com a função log-verossimilhança 
negativa, ou seja, com
$$
\mathcal{L}(\theta|X=x)=-log{(L(\theta|X))},
$$
nesse caso, se o valor da verossimilhança de uma amostra com muitas observações é um número positivo
muito pequeno, próximo de zero, o valor da log-verossimilhança negativa será um número positivo
numa escala mais fácil de trabalhar. 

Por outro lado, o fato da transformação incluir uma mudança de sinal implica que o comportamento da função de
log-verossimilhança negativa é oposto ao comportamento da função de verossimilhança. Isso significa que a 
hipótese com maior verossimilhança negativa terá menor log-verossimilhança.

# Estimadores de Máxima Verossimilhança

O método da Máxima Verossimilhança consiste em estimar os parâmetros de um modelo utilizando as estimativas
que tornam máximo o valor da função de verossimilhança. Isso é equivalente a encontrar o valor para o 
parâmetro que torna máxima a função de log-verossimilhança ou miníma a função log-verossimilhança negativa.
Assim, considere a definição seguinte:

_Definição:_ Para cada ponto amostral $x,$ seja $\hat{\theta}(x)$ um valor de parâmetro no qual $L(\theta|x)$
atinge seu máximo como uma função de $\theta,$ com $x$ mantido fixo. Um _estimador de máxima verossimilhança_
(EMV) do parâmetro $\theta$ com base em uma amostra $x$ é $\hat{\theta}(x).$

Para utilizar cálculos bivariados a fim de verificar que uma função $f(\theta_{1},\theta_{2})$ possui um 
máximo local em $(\hat{\theta}_{1},\hat{\theta}_{2}),$ deve ser mostrado que as seguintes três condições se 
mantêm.

i) As derivadas parciais de primeira ordem são 0,
$$
\dfrac{\partial}{\partial\theta_{1}}f(\theta_{1},\theta_{2})|_{\theta_{1}=\hat{\theta}_{1},\theta_{2}=
\hat{\theta}_{2}}=0\quad \textrm{e}\quad 
\dfrac{\partial}{\partial\theta_{2}}f(\theta_{1},\theta_{2})|_{\theta_{1}=\hat{\theta}_{1},\theta_{2}=
\hat{\theta}_{2}}=0
$$
ii) Pelo menos uma derivada de segunda ordem é negativa,
$$
\dfrac{\partial^{2}}{\partial\theta_{1}^{2}}f(\theta_{1},\theta_{2})|_{\theta_{1}=\hat{\theta}_{1},
\theta_{2}=\hat{\theta}_{2}}<0\quad \textrm{ou}\quad 
\dfrac{\partial^{2}}{\partial\theta_{2}^{2}}f(\theta_{1},\theta_{2})|_{\theta_{1}=\hat{\theta}_{1},
\theta_{2}=\hat{\theta}_{2}}<0
$$
iii) O jacobiano das derivadas parciais de segunda ordem é positivo,

$$
\left|\begin{array}{rrr}
\dfrac{\partial^{2}}{\partial\theta_{1}^{2}}f(\theta_{1},\theta_{2}) & \dfrac{\partial^{2}}{\partial\theta_{1}\partial\theta_{2}}f(\theta_{1},\theta_{2}) \\
\dfrac{\partial^{2}}{\partial\theta_{2}\partial\theta_{1}}f(\theta_{1},\theta_{2}) & \dfrac{\partial^{2}}{\partial\theta_{2}^{2}}f(\theta_{1},\theta_{2})
\end{array}\right|_{\theta_{1}=\hat{\theta}_{1},\theta_{2}=\hat{\theta}_{2}}=\dfrac{\partial^{2}}{\partial\theta_{1}^{2}}f(\theta_{1},\theta_{2})\dfrac{\partial^{2}}{\partial\theta_{2}^{2}}f(\theta_{1},\theta_{2})-\Big(\dfrac{\partial^{2}}{\partial\theta_{1}\partial\theta_{2}}f(\theta_{1},\theta_{2})\Big)^{2}|_{\theta_{1}=\hat{\theta}_{1},\theta_{2}=\hat{\theta}_{2}}>0
$$

Ou seja, para encontrar os estimadores de máxima verossimilhança são necessários todos esses cálculos. Lógico que para três ou mais parâmetros tais condições devem ser generalizadas. Além disso, tais condições satisfeitas garantem somente que os EMV são máximos locais e interiores, ainda é necessário garantir que são únicos e que não existe máximo na infinidade ou nos limites do espaço paramétrico. 

## Exemplo: Distribuição Normal
<p style="text-align: justify;">
Suponha que observamos o vetor $(13,12,15,17,16,11,18,16,13,15)$ de valores de uma variável $X\sim N(\mu,4).$ e digamos que há duas hipóteses competindo:

- Hipótese $A:$ a média de $X$ é $\mu=13;$
- Hipótese $B:$ a média de $X$ é $\mu=15;$

Precisamos responder qual o modelo (distribuição estatística) mais provável para se observar o vetor de valores acima? Sabemos que:
</p>
1- $X_{1},\cdots,X_{n},$ com $n=10,$ é uma amostra aleatória de $X\sim N(\mu,4);$

2- A densidade para cada observação é dada por $f(x_{i};\mu,\sigma^{2})=\dfrac{1}{2\sqrt{2\pi}}
\exp\Big\{-\dfrac{(x_{i}-\mu)^{2}}{8} \Big\};$

3- A verossimilhança é dada por $L(\mu)={\displaystyle \prod_{i=1}^{10}f(\mu;x_{i})};$

4- E a função log-verossimilhança é dada por:
$$
\begin{align}
L(\mu)=&-10\log{(2\sqrt{2\pi})}-\dfrac{1}{8}{\displaystyle \sum_{i=1}^{10}(x_{i}-\mu)^{2}}\\
=& -5\log{(8\pi)}-\dfrac{1}{8}\Big(\sum_{i=1}^{10}x_{i}^{2}-2\mu\sum_{i=1}^{10}x_{i}+10\mu^{2}\Big)
\end{align}
$$

Notemos que ao substituir os valores observados do vetor $X,$ obtemos uma função de $\mu.$ Vamos fazer isso e encontrar o gráfico de $l(\mu)$ para $\mu$ variando de 0 a 20; 

```{r,fig.align='center'}
x <- c(13,12,15,17,16,11,18,16,13,15)
sx2 <- sum(x^2) 
sx <- sum(x)
mu.vals <- seq(10, 20, l = 100)
lmu <- -5 * log(8 * pi) - (sx2 - 2 * mu.vals * sx + 10 * (mu.vals^2))/8
v=max(lmu)
plot(mu.vals, lmu, type = "l", xlab = expression(mu), ylab = expression(l(mu)))
abline(v=14.6,col="red")
```


Para obter o ponto de máximo calculamos a derivada de $l(\mu)$ em função de $\mu$ e encontramos:
$$
\dfrac{\partial l(\mu)}{\partial\mu}={\displaystyle \dfrac{1}{4}\sum_{i=1}^{10}(x_{i}-\mu)}
$$
Ao igualar a derivada acima a zero e resolver a equação, obtemos:
$$
\hat{\mu}={\displaystyle \dfrac{\sum_{i=1}^{10}x_{i}}{n}}
$$

Para provar que este é ponto de máximo, derivamos novamente e observamos que a segunda derivada no ponto $\hat{\mu}$ é sempre negativa.
$$
\dfrac{\partial^{2}l(\mu)}{\partial\mu^{2}}=-\dfrac{n}{\sigma^{2}}<0, \forall x\in \mathbb{R}.
$$


Observamos então que $x={\displaystyle \dfrac{\sum_{i=1}^{10}x_{i}}{10}}=14,6$ é o ponto de máximo de $L(\mu).$


Até este ponto, utilizei como referência os trabalhos de @Batista e @casella.

# Estudos Teóricos relacionados a Estimação de Parâmetros    
    
# Referências 

