---
title: "Modelo de Heckman"
author: "Fernando de Souza Bastos"
  #date: "`r format(Sys.time(), '%B %e, %Y')`"
date: "22 de outubro de 2025"
output:
  bookdown::html_document2:
    mathjax: 
      options:
        autoNumber: "AMS"
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    toc: yes
    #Sumário flutuante
    #toc_float: true
    #numerar seções
    number_sections: true
    #Mostrar ou esconder os códigos (show ou hide)
    code_folding: hide
    #Diversos modelos de documentos ver outros em http://bootswatch.com/
    theme: united
    header-includes:
      - \usepackage{amsmath}
      - \usepackage{dsfont}
      - \usepackage{multirow}
      - \usepackage{array}
      - \usepackage{bm}    
      - \usepackage{bbm}   
bibliography: bibfile.bib  
includes:
     keep_tex: yes
fontsize: 11pt
geometry: margin=1in
graphics: yes
#  pdf_document:
#    fig_caption: yes
#    keep_tex: yes
#    number_sections: yes
comments: yes
tags: [Viés de Seleção, Heckman, ssmodels, R]
---

***

```{r,echo = FALSE}
options(OutDec=",")
```


```{r setup, include=FALSE}
require(knitr)
require(kfigr)
library(kableExtra)
options(knitr.table.format = "latex")
knitr::opts_chunk$set(echo = TRUE,fig.align = "center",message=FALSE, 
                      warning=FALSE,fig.height=5, fig.width=7)
```

# Introdução

## Motivação

<p style="text-align: justify;">
Suponha que você deseje aplicar um modelo de regressão para explicar uma variável de interesse - por exemplo, produtividade, renda, teor de carbono ou eficiência técnica - utilizando um conjunto de covariáveis agronômicas. Entretanto, ao analisar o banco de dados, você percebe que parte das observações da variável resposta está ausente, e que essa ausência não é aleatória: ela pode estar associada a algum fator não observado, como adoção de tecnologia, decisão de plantio ou disponibilidade de informação.
</p>
<p style="text-align: justify;">
Nessas situações, a simples exclusão dos casos faltantes ou o uso de métodos tradicionais de regressão pode levar a resultados enviesados, pois as observações disponíveis não representam uma amostra aleatória da população. O **modelo de Heckman**, também conhecido como *modelo de seleção amostral* ou *Tobit tipo II*, foi desenvolvido justamente para lidar com esse tipo de viés, ao modelar simultaneamente dois processos:

 - (i) o mecanismo de seleção, que determina quando a variável resposta é observada, e  

 - (ii) a equação de interesse, que explica a variável resposta condicionalmente à seleção.

</p>
<p style="text-align: justify;">
A seguir, são apresentados **dez exemplos** inspirados em diferentes contextos das *ciências agrárias*, ilustrando cenários em que a aplicação do modelo de Heckman é apropriada. Em todos eles, a variável resposta $Y_{1i}$ é observada apenas quando uma condição latente $Y_{2i}^{*}>0$ é satisfeita, isto é, quando o processo de seleção permite a observação dos dados.
</p>
<p style="text-align: justify;">
1. **Produtividade de café.**  
A variável resposta $Y_{1i}$ representa a produtividade (kg/ha) das lavouras de café, observada apenas para produtores que colhem no ano (isto é, $Y_{2i}^{*}>0$ indica colheita realizada).  
Covariáveis: idade do cafezal, tipo de adubo, precipitação anual, experiência do produtor e altitude.
</p>
<p style="text-align: justify;">
2. **Renda agrícola.**  
$Y_{1i}$ é a renda líquida anual da propriedade, observada apenas para agricultores que comercializam sua produção ($Y_{2i}^{*}>0$).  
Covariáveis: tamanho da área cultivada, tipo de cultivo principal, acesso a crédito e escolaridade do produtor.
</p>
<p style="text-align: justify;">
3. **Peso médio de gado.**  
$Y_{1i}$ é o peso médio (kg) do rebanho, mensurado apenas em fazendas que participam de programas de pesagem ($Y_{2i}^{*}>0$).  
Covariáveis: raça predominante, tipo de pastagem, suplementação alimentar e manejo sanitário.
</p>
<p style="text-align: justify;">
4. **Eficiência técnica na irrigação.**  
$Y_{1i}$ mede a eficiência técnica estimada (0–1) de propriedades irrigadas, observada somente para aquelas que possuem sistema de irrigação ($Y_{2i}^{*}>0$).  
Covariáveis: área irrigada, tipo de solo, fonte de água e custo energético.
</p>
<p style="text-align: justify;">
5. **Produtividade de soja.**  
$Y_{1i}$ é a produtividade (t/ha) da soja, observada apenas em propriedades que efetivamente plantaram soja na safra ($Y_{2i}^{*}>0$).  
Covariáveis: tipo de semente, uso de inoculante, teor de matéria orgânica e regime de chuvas.
</p>
<p style="text-align: justify;">
6. **Taxa de sobrevivência de mudas florestais.**  
$Y_{1i}$ representa a taxa de sobrevivência (\%) de mudas após 12 meses, observada apenas nos projetos de reflorestamento que realizaram monitoramento ($Y_{2i}^{*}>0$).  
Covariáveis: espécie plantada, espaçamento, tipo de preparo do solo e presença de formigas cortadeiras.
</p>
<p style="text-align: justify;">
7. **Produção de leite.**  
$Y_{1i}$ é a produção média diária de leite (litros/vaca), observada somente para fazendas que registram controle leiteiro ($Y_{2i}^{*}>0$).  
Covariáveis: raça do gado, alimentação, temperatura média e número de ordenhas diárias.
</p>
<p style="text-align: justify;">
8. **Uso de defensivos agrícolas.**  
$Y_{1i}$ é o gasto anual com defensivos (R\$/ha), observável apenas para produtores que relatam uso desses insumos ($Y_{2i}^{*}>0$).  
Covariáveis: tipo de cultura, área plantada, presença de pragas e preço relativo dos produtos biológicos.
</p>
<p style="text-align: justify;">
9. **Teor de carbono no solo.**  
$Y_{1i}$ é o teor de carbono orgânico (g/kg) medido em propriedades que permitiram coleta de amostras ($Y_{2i}^{*}>0$).  
Covariáveis: tipo de solo, cobertura vegetal, sistema de manejo e altitude.
</p>
<p style="text-align: justify;">
10. **Rendimento de frutíferas.**  
$Y_{1i}$ é o rendimento médio de frutas (kg/planta), observado apenas para pomares em idade produtiva ($Y_{2i}^{*}>0$).  
Covariáveis: espécie cultivada, espaçamento, irrigação, adubação e ocorrência de pragas.
</p>

## Modelagem Teórica

<p style="text-align: justify;">
Considere o sistema de equações
\begin{align}
Y_{1i}^{*}&=\boldsymbol{x}_{i}^\top\boldsymbol{\beta}+\epsilon_{1i}, \tag{1.1}\label{1.1}\\
Y_{2i}^{*}&=\boldsymbol{z}_{i}^\top\boldsymbol{\gamma}+\epsilon_{2i},\quad i=1,\ldots,n, \tag{1.2}\label{1.2}
\end{align}
em que $Y_{1i}^{*}$ é a variável de interesse (latente) e $Y_{2i}^{*}$ rege o processo de seleção (também latente). Na prática, $Y_{1i}^{*}$ só é observado quando a condição de seleção é satisfeita. Especificamente, sabemos apenas se $Y_{2i}^{*}$ excede um limiar $a\in\mathbb{R}$ (fixo), o que gera observação seletiva de $Y_{1i}^{*}$. Observamos, portanto,
\vspace{-0.2cm}
\begin{align}\tag{1.3}\label{1.3}
U_{i}&=\mathbf{1}\{Y_{2i}^{*}>a\},\\
\nonumber Y_{i}&=Y_{1i}^{*}U_{i},\quad i=1,\ldots,n,
\end{align}
em que $\mathbf{1}\{\cdot\}$ é a função indicadora e $Y_{i}$ é não observado quando $U_{i}=0$. Os vetores de parâmetros $\boldsymbol{\beta}\in \mathbb{R}^{p}$ e $\boldsymbol{\gamma}\in \mathbb{R}^{q}$ são desconhecidos; os regressores $\boldsymbol{x}_{i}\in \mathbb{R}^{p}$ e $\boldsymbol{z}_{i}\in \mathbb{R}^{q}$ não precisam ser mutuamente exclusivos (embora, para identificação mais robusta, seja usual adotar ao menos uma variável em $\boldsymbol{z}_{i}$ que não apareça em $\boldsymbol{x}_{i}$ — a chamada **exclusion restriction**.
</p>

<p style="text-align: justify;">
Um ponto importante: ao escrevermos \(Y=Y_{1}^{*}\,U\) no modelo de Heckman, **não** estamos imputando o valor zero para as observações não selecionadas. Essa igualdade é apenas um **artifício teórico** para descrever que a variável observada \(Y\) tem distribuição **mista**: uma massa no ponto \(0\) quando \(U=0\) (isto é, quando o desfecho não é observado) e uma parte contínua quando \(U=1\). Na prática de estimação, mantemos \(Y\) como **NA** quando \(U=0\). No método de dois passos, o probit de seleção usa todas as linhas (\(U\in\{0,1\}\)), enquanto a regressão da equação de interesse é ajustada **apenas** na subamostra com \(U=1\), incluindo a razão inversa de Mills como regressor. Já na MLE conjunta, as linhas com \(U=0\) entram **somente** via o termo de probabilidade \( \log[1-\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})] \), e não como valores contínuos “zero” do desfecho. Portanto, \(Y=Y_{1}^{*}U\) resume a **regra de observação**, não uma imputação numérica no banco de dados.
</p>


### Índice latente, limiar e variável binária.

No modelo de seleção/probit, supõe-se a existência de uma **variável latente contínua**
\[
Y_{2i}^* \;=\; \boldsymbol z_i^\top \boldsymbol\gamma \;+\; \epsilon_{2i},
\]
que não é observada diretamente. O que observamos é apenas um indicador binário
\[
U_i \;=\; \mathbf{1}\{Y_{2i}^* > a\} \in \{0,1\},
\]
ou seja, $U_i=1$ se o **nível** do índice $Y_{2i}^*$ ultrapassa um limiar $a$ (fixo), e $U_i=0$ caso contrário. Portanto, o 0/1 observado provém do sinal (ou comparação com $a$) de uma grandeza contínua.

### Por que usar um latente contínuo? Argumentos.

 - Ordenação e decisão com ruído.
  
Em muitas aplicações (emprego, adoção de tecnologia, mensuração possível, etc.), a decisão ``entrar/sair'' é o resultado de um **balanço contínuo** entre ganhos e custos não totalmente observáveis. Modelamos esse balanço por $Y_{2i}^*$; a decisão observada é $U_i=1$ quando esse balanço supera o limiar $a$. O termo $\epsilon_{2i}$ captura fatores não observados (ruído) que afetam o índice.

 - Interpretação probabilística.
 
Assumindo $\epsilon_{2i}\sim \mathcal N(0,1)$ (normalização discutida abaixo), temos:
\[
\Pr(U_i=1 \mid \boldsymbol z_i) 
= \Pr(Y_{2i}^* > a \mid \boldsymbol z_i)
= \Pr\!\big(\epsilon_{2i} > a - \boldsymbol z_i^\top\boldsymbol\gamma\big)
= \Phi\!\big(\boldsymbol z_i^\top\boldsymbol\gamma - a\big),
\]
onde $\Phi(\cdot)$ é a CDF da Normal padrão. Assim, a probabilidade de $U_i=1$ é uma **função monótona** do índice linear $\boldsymbol z_i^\top\boldsymbol\gamma$. Isso conecta o modelo latente contínuo ao probit clássico.

 - Normalização de limiar ($a$) e intercepto.
 
O limiar $a$ é **não identificado separadamente** do intercepto de $\boldsymbol z_i^\top\boldsymbol\gamma$. Se escrevemos
\[
U_i=\mathbf{1}\{\boldsymbol z_i^\top\boldsymbol\gamma + \epsilon_{2i} > a\},
\]
então definir $\tilde\gamma_0=\gamma_0 - a$ (absorvendo $a$ no intercepto) leva a
\[
U_i=\mathbf{1}\{\tilde\gamma_0 + \boldsymbol z_{i,-0}^\top\boldsymbol\gamma_{-0} + \epsilon_{2i} > 0\}.
\]
Logo, sem perda de generalidade, **podemos fixar $a=0$** e estimar um intercepto.

 - Normalização de escala (variância do erro).
 
Somente o **sinal** de $Y_{2i}^*$ determina $U_i$. Se multiplicarmos toda a equação por $c>0$,
\[
c\,Y_{2i}^* = c\,\boldsymbol z_i^\top\boldsymbol\gamma + c\,\epsilon_{2i},
\quad
U_i = \mathbf{1}\{c\,Y_{2i}^* > c\,a\},
\]
as probabilidades não mudam (o sinal é preservado). Assim, a escala não é identificável; por convenção, fixa-se $\mathrm{Var}(\epsilon_{2i})=1$. Com isso, o link resultante é o probit; se, em vez de Normal, usarmos logística padrão, obtemos o logit.

 - Geometria e separação.
 
Para cada $\boldsymbol z_i$, o índice $Y_{2i}^*$ é uma variável contínua ao longo da reta real. A fronteira de decisão é o hiperplano 
$\{\boldsymbol z: \boldsymbol z^\top\boldsymbol\gamma = a\}$. O ruído $\epsilon_{2i}$ ``espalha'' a decisão em torno dessa fronteira, produzindo probabilidades estritamente entre 0 e 1, o que é desejável em presença de heterogeneidade não observada.

- Conexão com seleção amostral (Heckman).

No Heckman, existe **correlação** entre o erro da equação de interesse ($\epsilon_{1i}$) e o erro da seleção ($\epsilon_{2i}$), digamos $\rho\neq 0$. Essa correlação é a origem do viés de seleção: a decisão $U_i$ (0/1) depende do mesmo choque que afeta $Y_{1i}^*$ via o índice $Y_{2i}^*$. O aparato latente contínuo torna explícita essa estrutura conjunta (Normal bivariada), permitindo estimar $\rho$ e corrigir o viés.

O observado é $U_i\in\{0,1\}$; o determinante subjacente é um índice contínuo $Y_{2i}^*$. A probabilidade de $U_i=1$ é a probabilidade de o índice exceder o limiar: 
$\Pr(U_i=1\mid \boldsymbol z_i)=\Phi(\boldsymbol z_i^\top\boldsymbol\gamma-a)$.
Como $a$ e a escala do erro não são identificáveis separadamente, normaliza-se $a=0$ e $\mathrm{Var}(\epsilon_{2i})=1$.
Essa construção justifica formalmente o uso do probit/logit e explica por que, no Heckman, o parâmetro de correlação $\rho$ entre os erros das duas equações é central para o viés de seleção.


<p style="text-align: justify;">
Para definir o modelo de seleção amostral, assume-se que os termos de erro $(\epsilon_{1i},\epsilon_{2i})$ seguem distribuição normal bivariada:
\begin{align}
\tag{1.4} \label{1.4}
\begin{pmatrix}
\epsilon_{1i}\\
\epsilon_{2i}
\end{pmatrix} \overset{iid}{\sim} \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
0\\
0
\end{pmatrix},
\begin{pmatrix}
\sigma^{2} & \rho\,\sigma \\
\rho\,\sigma & 1
\end{pmatrix}
\end{bmatrix},\quad i=1,\ldots,n,
\end{align}
em que $\sigma^{2}_{1}\equiv \sigma^{2}$. Como $Y_{2i}^{*}$ não é observado, a sua variância não é identificável em escala absoluta; por convenção, fixa-se $\sigma_{2}^{2}=1$ sem perda de generalidade (qualquer normalização positiva equivaleria a reescalar $\pmb{\gamma}$ e o limiar $a$). De forma análoga, usualmente toma-se $a=0$, pois valores distintos podem ser absorvidos pelo intercepto da equação de seleção \ref{1.1}.
</p>
<p style="text-align: justify;">
O sistema \ref{1.1}–\ref{1.2} é conhecido como *modelo de Heckman*, *Tobit tipo 2* ou *modelo de seleção amostral*. A equação \ref{1.1} é a *equação de interesse primária* (ou de regressão), enquanto \ref{1.2} é a *equação de seleção*. O parâmetro de correlação $\rho\in(-1,1)$ desempenha papel central: quando $\rho\neq 0$, há dependência entre os choques que afetam o desfecho $Y_{1i}^{*}$ e o mecanismo que regula sua observação, produzindo \textbf{viés de seleção} se a amostra for analisada ignorando o processo de seleção. Em contraste, quando $\rho=0$, a seleção é *ignorável* sob a suposição de normalidade, e a regressão na subamostra observada fornece estimativas não viesadas de $\pmb{\beta}$.
</p>
<p style="text-align: justify;">
Do ponto de vista inferencial, duas abordagens clássicas são frequentes: (i) a estimação por *máxima verossimilhança* conjunta, baseada na densidade normal bivariada, e (ii) o procedimento em *dois estágios* de Heckman, em que se estima primeiro a equação de seleção via *probit* e, em seguida, incorpora-se a *razão de Mills inversa* como regressor adicional na equação de interesse. A primeira é assintoticamente eficiente sob correta especificação; a segunda é simples e interpretável, servindo como diagnóstico prático do viés de seleção [@Heckman1976; @Heckman1979].
</p>
<p style="text-align: justify;">
Aplicações do modelo são vastas - de economia do trabalho (salários observados apenas para quem está empregado) a estudos em ciências agrárias (produtividade ou renda observadas somente para produtores que adotam determinada tecnologia, propriedades com mensuração completa apenas quando acessíveis etc.). Em tais contextos, a não observação depende de decisões ou condições correlacionadas com o desfecho de interesse, tornando imprescindível modelar conjuntamente seleção e resultado para evitar conclusões enviesadas.
</p>
<p style="text-align: justify;">
Neste trabalho, apresentamos uma aplicação do modelo de Heckman em \textsf{R}, enfatizando: (a) especificação e identificação (normalização, papel de variáveis de exclusão); (b) estimação por máxima verossimilhança e por dois estágios; (c) diagnóstico do viés de seleção via teste sobre $\rho$; e (d) interpretação substantiva dos coeficientes e efeitos marginais. Implementamos os exemplos com o pacote \texttt{ssmodels}, discutindo detalhadamente a replicação dos resultados, a sensibilidade às suposições e boas práticas para relatórios reprodutíveis em \textsf{R Markdown}.
</p>

## Método de Dois Passos

<p style="text-align: justify;">
Um procedimento de estimação também proposto por \cite{heckman1979} foi o método de dois passos. Esse método, foi sugerido como um bom estimador para pontos de partida confiáveis e eficientes na estimação por máxima verossimilhança \citep{Leung2000}. O método é baseado no fato da média 
condicional $\tilde{\mu}_{i}=E(y_{i}|y_{1i}^{*}\ \textrm{é observado},\pmb{x}_{i},\pmb{z}_{i}),\ \textrm{para}\ i=1,\cdots,n,$ 
ser dada por
\begin{eqnarray}
\nonumber \tilde{\mu}_{i}&=&E(y_{i}|\ y_{1i}^{*}\ \textrm{é observado},\pmb{x}_{i},\pmb{z}_{i})\\
\nonumber                &=&E(y_{i}|\ y_{2i}^{*}>0,\pmb{x}_{i},\pmb{z}_{i})\\
\nonumber                &=&E(y_{1i}^{*}|\ \pmb{z}_{i}^\top\pmb{\gamma}+\epsilon_{2i}>0,\pmb{x}_{i},\pmb{z}_{i})\\
\nonumber                &=&E(\pmb{x}_{i}^\top\pmb{\beta}+\epsilon_{1i}|\epsilon_{2i}>-\pmb{z}_{i}^\top\pmb{\gamma},
\pmb{x}_{i},\pmb{z}_{i})\\
\nonumber                &=&\pmb{x}_{i}^\top\pmb{\beta}+E(\epsilon_{1i}|\epsilon_{2i}>-\pmb{z}_{i}^\top\pmb{\gamma}, \pmb{z}_{i})\\
\nonumber                &=& \mathbf{x}_{i}^\top\pmb{\beta} + \rho\sigma\lambda(-\mathbf{z}_{i}^\top\pmb{\gamma})\\
\nonumber                &=& \pmb{x}_{i}^\top\pmb{\beta} + \rho\sigma\dfrac{\phi(\pmb{z}_{i}^\top\pmb{\gamma})}
{\Phi(\pmb{z}_{i}^\top\pmb{\gamma})}\\
                         &=& \pmb{x}_{i}^\top\pmb{\beta}+\lambda_{i}\beta_{\lambda},\tag{1.5}\label{1.5}
\end{eqnarray}
em que $\lambda_{i}=\dfrac{\phi(\pmb{z}_{i}^\top\pmb{\gamma})}{\Phi(\pmb{z}_{i}^\top\pmb{\gamma})}$ denota a razão inversa de Mills, 
$\beta_{\lambda}=\rho\sigma,\ \rho$ é a correlação entre $\epsilon_{1i}$ e $\epsilon_{2i}$ e $\sigma$ é o desvio padrão 
de $\epsilon_{1i}.$ A partir de \ref{1.5} podemos reescrever a equação de interesse como
\begin{align}\tag{1.6}\label{1.6}
\tilde{y}_{i}=\tilde{\mu}_{i}+\varepsilon_{i},
\end{align}
em que $\tilde{\mu}_{i}$ é dada em (\ref{1.5}) e $\varepsilon_{i}$ é um novo termo de erro de média zero e 
independente de $\pmb{z}_{i}$ e de $\pmb{x}_{i}$. O termo $\lambda_{i}\beta_{\lambda}$ em (\ref{1.5}) explica 
a inconsistência do estimador de mínimos quadrados ordinários (MQO), quando $\rho\neq 0,$ e se MQO fosse utilizado para 
encontrar as estimativas dos parâmetros de (\ref{1.2}). A partir daí, o primeiro passo do método é ajustar o 
modelo probit a equação de seleção (\ref{1.1}) e estimar $\widehat{\pmb{\gamma}}$ e 
$\widehat{\lambda}_{i}=\frac{\phi(\pmb{z}_{i}^\top\pmb{\widehat{\gamma}})}{\Phi(\pmb{z}_{i}^\top
\pmb{\widehat{\gamma}})}.$ Em um segundo passo, estimamos por MQO os parâmetros 
$\pmb{\beta}$ e $\beta_{\lambda}=\rho\sigma$ de \ref{1.6} usando os valores de $y_{1i}^{*}$ observados. 
Um estimador para a variância de $\epsilon_{1}$ é dado por
\begin{align}
\widehat{\sigma}^{2}=\dfrac{1}{n_{u}}\left(\pmb{\widehat{\varepsilon}}^\top\pmb{\widehat{\varepsilon}}+
\widehat{\beta}_{\lambda}^{2}{\displaystyle \sum_{i=1}^{n_{u}}\widehat{\delta}_{i}}\right),
\tag{1.7}\label{1.7}
\end{align}
em que $\widehat{\varepsilon}$ é o vetor residual da estimação de MQO de (\ref{1.6}), $n_{u}$ é o número de observações nesta estimação e $\widehat{\delta}_{i}=\widehat{\lambda}_{i}(\widehat{\lambda}_{i}+\pmb{z}_{i}^\top
\pmb{\gamma}).$ Finalmente, um estimador para a correlação entre $\epsilon_{1}$ e $\epsilon_{2}$ é dado por 
\begin{align}
\widehat{\rho}=\dfrac{\widehat{\beta}_{\lambda}}{\widehat{\sigma}},
\tag{1.8}\label{1.8}
\end{align}
nesse caso, $\widehat{\rho}$ pode estar fora do intervalo $[-1,1].$
</p>
<p style="text-align: justify;">
A maior vantagem do método de dois passos é sua simplicidade, pois é mais fácil de ajustar do que o método de máxima 
verossimilhança, não requer algoritmos complicados e é uma alternativa mais robusta. Porém, é menos eficiente e o uso da 
razão inversa de Mills $(\lambda)$ pode ocasionar possíveis problemas de multicolinearidade devido a sua linearidade em 
grande parte do seu suporte, como é possível observar na Figura \ref{fig:Mills}. Para diminuir este problema é sugerido 
a \textbf{restrição de exclusão}, de acordo com a qual, pelo menos uma variável, que é um bom preditor de $Y_{2}^{*}$ e está 
incluída na equação de seleção, não deve aparecer na regressão primária. 
</p>
\begin{figure}[H]
\centering{
\resizebox{16cm}{7cm}{\input{plot_Inv_Mills.tex}}} %% assuming figurefile is a tikz code
%\captionsetup{justification=centering,margin=1cm}
\vspace*{-15mm}
\caption{Razão inversa de Mills para $z^\top\gamma\in [-5,5].$ }
\tag{1.9}\label{1.9}
\end{figure}

## Exemplo Prático


<p style="text-align: justify;">
O conjunto de dados analisado é composto por **99 observações** e **13 variáveis**, provenientes de um experimento que avaliou características físicas e químicas de agregados do solo sob diferentes condições de textura e umidade. Cada linha do banco representa uma unidade experimental identificada pela variável `ID`, que combina o tipo de solo (`Soil_type.x`), a condição de umidade (`Soil_moisture.x`) e o número de repetição (`Rep`). A variável `Days.x` indica o tempo de incubação em dias. As colunas `Prop_less_than_53_um`, `Prop_250_53_um`, `Prop_2000_250_um` e `Prop_greater_than_2mm` expressam, respectivamente, as proporções de agregados em diferentes classes de tamanho (menores que 53 μm, entre 53 μm e 250 μm, entre 250 μm e 2 mm, e maiores que 2 mm). A variável `Mean_weight_diameter` corresponde ao diâmetro médio ponderado dos agregados, um indicador da estabilidade estrutural do solo. Já as variáveis `Extractable_organic_C`, `Microbial_biomass_C` e `Total_aggregate_associated_C` representam, respectivamente, o carbono orgânico extraível, o carbono da biomassa microbiana e o carbono total associado aos agregados. Em conjunto, essas variáveis permitem investigar como a textura e a umidade do solo influenciam a distribuição de agregados e o acúmulo de carbono em diferentes frações do solo.
</p>
<p style="text-align: justify;">
Suponha agora que, ao tentar ajustar um modelo de regressão para explicar o carbono total associado aos agregados (`Total_aggregate_associated_C`), o pesquisador perceba que essa variável não foi medida em todas as amostras. Em particular, o carbono total pode ter sido determinado apenas nos solos com teor de umidade mais estável (`Soil_moisture.x = "Steady"`) ou naqueles que apresentaram frações mais estruturadas, com maior proporção de agregados grossos (`Prop_greater_than_2mm`). Nesse caso, as observações de $Y_{1i} =$ `Total_aggregate_associated_C` não seriam aleatórias: a ausência de valores dependeria de um *mecanismo de seleção* relacionado às próprias características do solo.
</p>
<p style="text-align: justify;">
Modelos lineares tradicionais ignoram esse tipo de dependência e, por isso, podem produzir estimativas enviesadas dos efeitos das covariáveis sobre o carbono total. Para contornar esse problema, pode-se aplicar o **modelo de Heckman**, que considera simultaneamente duas equações:  
(i) uma *equação de seleção*, que modela a probabilidade de o carbono total ser observado (por exemplo, em função de `Soil_type.x`, `Soil_moisture.x` e `Mean_weight_diameter`), e  
(ii) uma *equação de resultado*, que explica o valor de `Total_aggregate_associated_C` condicionalmente à observação, usando covariáveis como `Prop_greater_than_2mm`, `Extractable_organic_C` e `Microbial_biomass_C`.
</p>
<p style="text-align: justify;">
Essa abordagem permite avaliar, de forma estatisticamente consistente, como diferentes atributos físicos e biogeoquímicos do solo influenciam o acúmulo de carbono, ao mesmo tempo em que corrige o **viés de seleção** decorrente da mensuração seletiva das amostras. Assim, o modelo de Heckman oferece um arcabouço robusto para estudos em que parte das variáveis de interesse é observada apenas em condições específicas do experimento.
</p>

```{r tabela-heckman, echo=FALSE, message=FALSE, warning=FALSE}
library(kableExtra)
library(tidyverse)
library(sampleSelection)
library(ssmodels)
# Lê o CSV com atenção para strings vazias ou lixo
df <- read.csv("data/Incubation_data.csv",
               na.strings = c("", "NA", "?", "NULL", "null", "–", " ", "-", "N/A"))
# --- 1) Limpeza dos -9999 (como antes) ---
clean_neg9999 <- function(data, cols) {
  data <- as.data.frame(data)
  for (cc in cols) {
    if (cc %in% names(data)) {
      v <- data[[cc]]
      data[[cc]][!is.na(v) & (v == -9999 | v == -9999.0)] <- NA
    } else {
      warning(sprintf("Coluna '%s' não encontrada.", cc))
    }
  }
  data
}

df <- clean_neg9999(df, cols = "Total_aggregate_associated_C")

# Fatores
df$Soil_type.x     <- factor(df$Soil_type.x,     levels = c("Sandy","Loamy","Clayey"))
df$Soil_moisture.x <- factor(df$Soil_moisture.x, levels = c("Steady","Transient"))

# Seleção: 1 se há valor observado no outcome
df$sel <- as.integer(!is.na(df$Total_aggregate_associated_C))

# (opcional) log do desfecho se todos > 0
df$log_TAAC <- ifelse(!is.na(df$Total_aggregate_associated_C) &
                            df$Total_aggregate_associated_C > 0,
                          log(df$Total_aggregate_associated_C), NA_real_)

# --- 2) FÓRMULAS CORRIGIDAS 
sel_form <- sel ~ Soil_type.x + Days.x + Mean_weight_diameter
out_form <- log_TAAC ~ Soil_type.x + Soil_moisture.x + Mean_weight_diameter

# --- 1) selecione as variáveis-chave do modelo ---
vars_sel  <- c("sel", "Soil_type.x", "Days.x", "Mean_weight_diameter")
vars_out  <- c("log_TAAC", "Soil_moisture.x", "Total_aggregate_associated_C") # mostra o original também

vis_df <- df |>
  select(ID, all_of(vars_sel), all_of(vars_out)) |>
  slice_head(n = 10)  # mostre as 10 primeiras linhas na Introdução

# formato bonito e portável (PDF/HTML)
kbl(
  vis_df,
  booktabs = TRUE,
  caption = "Amostra (10 primeiras linhas) das variáveis usadas no modelo de Heckman."
) |>
  kable_styling(full_width = FALSE, position = "center") |>
  add_header_above(c(" " = 1, "Seleção" = length(vars_sel), "Desfecho" = length(vars_out))) |>
  footnote(
    general = "sel = 1 se o desfecho está observado. log_TAAC = log(Total_aggregate_associated_C) quando > 0.",
    threeparttable = TRUE
  )
```


```{r resumo-variaveis, echo=FALSE}
numeric_vars <- c("Days.x", "Mean_weight_diameter", "Total_aggregate_associated_C", "log_TAAC")
factor_vars  <- c("Soil_type.x", "Soil_moisture.x", "sel")

# Resumo numérico
res_num <- df |>
  select(any_of(numeric_vars)) |>
  pivot_longer(everything(), names_to = "Variavel", values_to = "valor") |>
  group_by(Variavel) |>
  summarise(
    n        = sum(!is.na(valor)),
    n_miss   = sum(is.na(valor)),
    media    = mean(valor, na.rm = TRUE),
    desvio   = sd(valor, na.rm = TRUE),
    min      = min(valor, na.rm = TRUE),
    max      = max(valor, na.rm = TRUE),
    .groups = "drop"
  )

kbl(res_num, digits = 3, booktabs = TRUE,
    caption = "Resumo numérico das variáveis contínuas usadas no modelo.") |>
  kable_styling(full_width = FALSE, position = "center")

# Resumo categórico (top contagens)
res_cat <- df |>
  select(any_of(factor_vars)) |>
  mutate(across(all_of(factor_vars), ~as.character(.))) |>
  pivot_longer(everything(), names_to = "Variavel", values_to = "nivel") |>
  filter(!is.na(nivel)) |>
  count(Variavel, nivel, name = "freq") |>
  group_by(Variavel) |>
  arrange(desc(freq), .by_group = TRUE) |>
  slice_head(n = 6) |>
  ungroup()

kbl(res_cat, booktabs = TRUE,
    caption = "Distribuição (top níveis) das variáveis fator/indicador.") |>
  kable_styling(full_width = FALSE, position = "center")
```

No procedimento de **dois passos** de Heckman, o **primeiro passo** consiste em estimar a *equação de seleção* via modelo probit, exatamente o que faz o bloco bloco abaixo. 

```{r probit1, echo=TRUE}
#Modelo Probit
fit1<-glm(sel ~ Soil_type.x + Days.x + Mean_weight_diameter,family=binomial(link=probit),data=df)
```

Nesse ajuste, modelamos a variável indicadora $(U_i=\mathbf{1}\{Y_{2i}^*>0\})$ com $\Pr(U_i=1\mid \mathbf{z}_i)=\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})$, em que ($\Phi(\cdot)$) é a CDF da Normal padrão e ($\mathbf{z}_i=(\texttt{Soil_type.x},\ \texttt{Days.x},\ \texttt{Mean_weight_diameter})$). A estimação por `glm(..., family = binomial(link = probit))` fornece o preditor linear ($\hat\eta_i=\mathbf{z}_i^\top\hat{\boldsymbol{\gamma}}$) e as probabilidades previstas ($\hat p_i=\Phi(\hat\eta_i)$), permitindo calcular, para as observações selecionadas ((U_i=1)), a **razão de Mills inversa** ($\hat\lambda_i=\phi(\hat\eta_i)/\Phi(\hat\eta_i)$), com ($\phi(\cdot)$) a densidade da Normal padrão. 


```{r probit2, echo=FALSE}
#Modelo Probit
fit1<-glm(sel ~ Soil_type.x + Days.x + Mean_weight_diameter,family=binomial(link=probit),data=df)
summary(fit1)
# 1) Preditor linear por linha (η = Xβ)
eta  <- predict(fit1, type = "link")      # vetor n×1
df$Zgamma <- eta
# 2) CDF e PDF da Normal padrão
Phi  <- pnorm(eta) 
phi  <- dnorm(eta)
df$Phi  <- pnorm(eta)                        # Φ(η)
df$phi  <- dnorm(eta)                        # φ(η)

# 3) Razão de Mills inversa para selecionados (U=1)
#    λ_i = φ(η_i) / Φ(η_i). Use um eps para evitar divisão por ~0.
eps  <- .Machine$double.eps
df$IMR  <- ifelse(df$sel == 1, phi / pmax(Phi, eps), NA_real_)

# (opcional) probabilidade prevista de seleção
p_hat <- predict(fit1, type = "response") # = Φ(η)

```

```{r tabela-heckman-atualizada, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(kableExtra)

# garantir que p_hat está no data.frame
df$p_hat <- predict(fit1, type = "response")

# seleções (mesmas da sua tabela anterior)
vars_sel  <- c("sel", "Soil_type.x", "Days.x", "Mean_weight_diameter")
vars_out  <- c("log_TAAC", "Soil_moisture.x", "Total_aggregate_associated_C")

# novas colunas calculadas no probit2
vars_new  <- c("Zgamma", "Phi", "phi", "IMR", "p_hat")

# montar visualização (primeiras 10 linhas)
vis_df2 <- df |>
  select(ID, all_of(vars_sel), all_of(vars_out), all_of(vars_new)) |>
  mutate(
    across(any_of(c("Zgamma","Phi","phi","IMR","p_hat")), ~round(., 4))
  ) |>
  filter(sel == 1) |> slice_head(n = 10)

kbl(
  vis_df2,
  booktabs = TRUE,
  caption = "Amostra (10 primeiras linhas) com covariáveis do modelo e colunas derivadas do probit: Xβ, Φ(η), φ(η), IMR e p̂."
) |>
  kable_styling(full_width = FALSE, position = "center") |>
  add_header_above(c(" " = 1,
                     "Seleção" = length(vars_sel),
                     "Desfecho" = length(vars_out),
                     "Derivadas do Probit (1º passo)" = length(vars_new))) |>
  footnote(
    general = "Xβ = preditor linear (η); Φ(η) = probabilidade prevista; φ(η) = densidade; IMR = φ(η)/Φ(η) para sel=1; p̂ = Φ(η).",
    threeparttable = TRUE
  )
```

No **segundo passo**, incluímos ($\hat\lambda_i$) como regressor adicional na equação de interesse para corrigir o viés de seleção e obter estimativas consistentes dos coeficientes; é recomendável, para identificação mais robusta, que ao menos uma variável apareça em ($\mathbf{z}_i$) e não em ($\mathbf{x}_i$) (restrição de exclusão).

```{r outcome, echo=TRUE, warning=FALSE,message=FALSE}
#Modelo de Regressão Linear Simples com wage>0
fit2<- lm(log_TAAC ~ Soil_type.x + Soil_moisture.x + Mean_weight_diameter+IMR, data = df[df$sel==1,])
summary(fit2)
```

# Método da Máxima Verossimilhança

Para estimar os parâmetros por máxima verossimilhança, precisamos da distribuição da v.a. mista \(Y_i=Y_{1i}^* U_i\), em que \(U_i=\mathbf{1}\{\mathbf{z}_i^\top\boldsymbol{\gamma}+\epsilon_{2i}>0\}\) e
\[
\begin{pmatrix}\epsilon_{1i}\\ \epsilon_{2i}\end{pmatrix}\sim\mathcal{N}\!\left(\mathbf{0},
\begin{pmatrix}\sigma^2 & \rho\sigma\\ \rho\sigma & 1\end{pmatrix}\right).
\]

Pela lei das probabilidades totais,
\[
P(Y_i\le y)=P(U_i=1,\,Y_{1i}^*\le y)+P(U_i=0,\,0\le y).
\]

Assim, \(Y_i\) tem um **componente discreto** (massa no ponto \(0\), quando \(U_i=0\)) e um **componente contínuo** (quando \(U_i=1\)). O componente discreto é dado pelo probit:
$$
P(U_i=u)=\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})^{u}\,\big[1-\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})\big]^{1-u},\quad u\in\{0,1\}.
$$

Para o componente contínuo, vale a identidade de seleção condicional (Arellano-Valle & Genton):
$$
f_{Y_{1i}^{*}\,|\,U_i=1}(y;\boldsymbol{\theta})
= f_{Y_{1i}^{*}}(y;\boldsymbol{\theta})\,
\dfrac{P(U_i=1\,|\,Y_{1i}^{*}=y;\boldsymbol{\theta})}{P(U_i=1)}.
\tag{2.1}\label{eq:dens_cont_heckman}
$$

Da normalidade bivariada,
$$
Y_{2i}^{*}\mid Y_{1i}^{*}=y \sim \mathcal{N}\!\left(
\mathbf{z}_i^\top\boldsymbol{\gamma}+\frac{\rho}{\sigma}\,(y-\mathbf{x}_i^\top\boldsymbol{\beta})\ ,\
1-\rho^{2}\right),
$$
logo
$$
P(U_i=1\,|\,Y_{1i}^{*}=y)=
\Phi\!\left(\frac{\mathbf{z}_{i}^\top\boldsymbol{\gamma}}{\sqrt{1-\rho^{2}}}
+\frac{\rho\,(y-\mathbf{x}_{i}^\top\boldsymbol{\beta})}{\sigma\sqrt{1-\rho^{2}}}\right).
\tag{2.2}\label{eq:cond_prob}
$$

Também,
$$
f_{Y_{1i}^{*}}(y)=\frac{1}{\sigma}\,
\phi\!\left(\frac{y-\mathbf{x}_i^\top\boldsymbol{\beta}}{\sigma}\right).
$$

Substituindo \ref{eq:cond_prob} em \ref{eq:dens_cont_heckman} e usando \(P(U_i=1)=\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})\), obtemos o componente contínuo:
$$
f_{Y_{1i}^{*}\,|\,U_i=1}(y;\boldsymbol{\theta})
=\frac{1}{\sigma\,\Phi(\mathbf{z}_{i}^\top\boldsymbol{\gamma})}\,
\phi\!\left(\frac{y-\mathbf{x}_{i}^\top\boldsymbol{\beta}}{\sigma}\right)\,
\Phi\!\left(\frac{\mathbf{z}_{i}^\top\boldsymbol{\gamma}}{\sqrt{1-\rho^{2}}}
+\frac{\rho\,(y-\mathbf{x}_{i}^\top\boldsymbol{\beta})}{\sigma\sqrt{1-\rho^{2}}}\right).
$$

#### Log-verossimilhança

Para o par \((Y_i,U_i)\), a contribuição da log-verossimilhança é
$$
\mathcal{L}_i(\boldsymbol{\theta})
= u_i\Big\{\log f_{Y_{1i}^{*}\,|\,U_i=1}(y_i;\boldsymbol{\theta})\Big\}
+ u_i\log \Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})
+ (1-u_i)\log\!\big[1-\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})\big],
$$
equivalentemente,
$$
\mathcal{L}_i(\boldsymbol{\theta})
= u_i\left\{-\log\sigma+\log\phi\!\left(\frac{y_i-\mathbf{x}_i^\top\boldsymbol{\beta}}{\sigma}\right)
+\log\Phi\!\left(\frac{\mathbf{z}_{i}^\top\boldsymbol{\gamma}+\rho\,\frac{y_i-\mathbf{x}_i^\top\boldsymbol{\beta}}{\sigma}}{\sqrt{1-\rho^{2}}}\right)\right\}
+(1-u_i)\log\!\big[1-\Phi(\mathbf{z}_i^\top\boldsymbol{\gamma})\big].
$$

Sob a suposição de normalidade bivariada, os EMV obtidos ao maximizar \(\mathcal{L}(\boldsymbol{\theta})=\sum_{i=1}^{n}\mathcal{L}_i(\boldsymbol{\theta})\) são consistentes, assintoticamente normais e eficientes. Como \(\mathcal{L}(\boldsymbol{\theta})\) é não linear e pode ter ótimos locais, emprega-se otimização numérica (e.g., BFGS), tipicamente iniciada com bons valores do método de dois passos.


```{r tobit, echo=TRUE}
theta_HC <- HeckmanCL(selection = sel_form, outcome = out_form, data = df)
summary(theta_HC)
```


# Referências 

